---
title: "Denoising and decontaminating v2"
output: html_notebook
---
After running the demultiplexer_for_dada2 (http://github.com/ramongallego/demultiplexer_for_dada2), we have to denoise the whole dataset. We will do this by using 4 different processes:


  * **Estimation of *Tag-jumping* or indices *cross-talk* **. We run multiple samples on each MiSeq run. These are identified by two sets of molecular barcodes. There is the potential of some sequences to be assigned to the wrong sample, which is a bummer. To estimate how many reads did this, on each MiSeq run we added some samples whose composition is known and extremely unlikely to be present in the enviromental samples studied. AS a result of this **Tag-jumping**, some of the positive control sequences might show in the environmental samples and viceversa. In our case, these positive controls are made of either Kangaroo or Ostrich (and Alligator). The process consists on, for each run, to model the compositon observed on the positive controls and substract it from the environmental samples from that run. The output will be a dataset with the same number of samples as before, but with fewer reads of certain sequences (ASVs)
  
  * **Discarding samples with extremely low number of reads**. Sometimes the number of reads sequenced from a particular replicate are really low, and hence the relative proportions of ASVs would be skewed. 
  
  * **Full clearance from Positive control influence**. THis process also takes advantage of the known composition of the positive controls. Each ASV found in the positive controls with a higher abundace in them than in the rest of the samples will be labelled as  **Positive** and removed from the environmental dataset. The output will be a dataset with the same number of samples as before but with fewer ASVs.
  
  * **Occupancy modelling** . Is the presence of a ASV a reflection of a biological reality or likely a PCR artifact? This may seem trivial in extreme cases (an ASV that only appears in one PCR replicate in the whole dataset) but how to discriminate between PCR artifacts from rare but real organisms? We use Occupancy modelling to determine if the pattern of presence of a ASV in a dataset reflects that. The output of this procedure will be a datasetwith the same number of samples as before but with fewer ASVs.
  
  * **Dissimilarity between PCR replicates**. The workflow that leads to the sequencing of a particular sample is subject to many stochatic processes, and it is not unlikely that the composition retrieved is very different for the original community. A way to ensure that this difference is minimal is through the separate analysis of each PCR replicate. We used that approach and modeled the dissimilarity between each PCr replicate and the group centroid. This way of modeling the dissimilarity allows us to discard those PCR replicate that won't fit the normal distribution of dissimilarities. The output of this procedure will be a dataset with the same number of **Hashes** as before but with fewer **samples**.
  
  
As with everything, we will start the process by loading the required packages and datasets.

# Load the dataset and metadata



```{r load libraries, include=FALSE}
 knitr::opts_chunk$set(warning = FALSE)

 library (tidyverse)
 library (vegan)
 #library (MASS)
 library (proxy)
library(reshape2)

```

We will load the ASV table and the metadata file. They are in the same folder so we use `list.files` to access them and a neat combination of `bind.rows` and `map(read_csv)`

```{r load datasets - we will be doing that for all runs}



all.asvs     <- list.files (path ="..", pattern = "^ASV_table_r", recursive = T, full.names = T)
all.metadata <- list.files (path ="..", pattern = "^metadata_r", recursive = T, full.names = T)
all.hashes   <- list.files (path ="..", pattern = "^Hash_key_r", recursive = T, full.names = T,ignore.case = T)

ASV.table <- bind_rows(map(all.asvs, read_csv), .id = "Miseq_run")

metadata <- bind_rows(map(all.metadata, function(x) {
  read_csv(x) %>%
    dplyr::select("sample_id", "pri_index_name", "Tag"= "sec_index_name")
  }),.id = "Miseq_run")

Hash.key <- bind_rows(map(all.hashes, read_csv))

Hash.key %>% 
  distinct(Hash, .keep_all = T) -> Hash.key
```


## Data Cleanup - Don't act like you don't need this

Emily already cleaneup up the data. A few things we check for: That **no sample appears twice** in the metadata. That the metadata **uses Tag_01 instead of Tag_1** (so it can be sorted alphabetically). That **the structure** Site_YYYYMM[A-C].[1-3] **is the same** across the dataset.

```{r data cleaning}

# Check that no sample appears more than once in the metadata

metadata %>% 
  group_by(sample_id) %>%
  summarise(tot = n()) %>% 
  arrange(desc(tot)) # Samples only appear once


# We should change Tag_1 for Tag_01

metadata %>%
  mutate(Tag = case_when(str_detect(Tag, "\\_[0-9]{1}$")       ~     str_replace(Tag, "Tag_", "Tag_0"),
                         TRUE                                  ~     Tag  )) -> metadata



metadata %>% mutate(x= str_count(sample_id, pattern = "_")) %>% arrange(desc(x)) # Max number of underscores is 5

metadata %>% mutate(x= str_count(sample_id, pattern = "_")) %>% arrange((x)) # Min number of underscores is also 5

   
ASV.table %>% distinct(sample) %>% mutate(x= str_count(sample, pattern = "_")) %>% arrange(desc(x)) # Also in the ASV table

ASV.table %>% distinct(sample) %>% mutate(x= str_count(sample, pattern = "_")) %>% arrange((x)) # Also in the ASV table

```

Now let's check the positive controls: are all for each run kangaroos or also ostriches
```{r Cheking Ks and Os}

metadata %>% 
  filter(str_detect(sample_id, "NA"))

```
Looking at the metadata - it used Ostriches in Round 1 and Kangaroo in rounds 2-5


The output of this process are a clean ASV table and a clean metadata file.

## Cleaning Process 1: Estimation of *Tag-jumping* or sample *cross-talk*

Before we modify our datasets on any way, we can calculate how many sequences that were only supposed to be in the positives control appeared in the environmental samples, and how many did the opposite. First we divide the dataset into positive control and environmental samples. Also create an ordered list of the Hashes present in the positive controls, for ease of plotting

```{r split into two}

ASV.table %>%  mutate(source = case_when(str_detect(sample, "NA_")    ~   "Positives",
                                         TRUE                                                ~   "Samples")) -> ASV.table

ASV.table %>% 
  filter (source == "Positives") %>% 
  group_by(Hash) %>% 
  summarise(tot = sum(nReads)) %>% 
  arrange(desc(tot)) %>% 
  pull(Hash) -> list.of.hashes.in.positive


```

Now let's create a jumping vector. What proportion of the reads found in the positives control came from elsewhere?, and what proportion of the reads in the samples came from the positives control?

### Step 1: Nest the dataset and split it in positives and samples

To streamline the process and make it easier to execute it similarly but independently on each Miseq run, we nest the dataset by run. 
So Step1 is create a nested table so we can run this analysis on each run independently. 


```{r nesting the dataset}

ASV.table %>% 
  group_by(Miseq_run, source) %>% 
  nest() %>% 
  spread(source, data) -> ASV.nested 
```

That wasn't too complicated. Let's start a summary function that keeps track of our cleaning process

```{r summary.file}

how.many <- function(ASVtable, round){
  ASVtable %>% ungroup() %>% 
    summarise(nsamples = n_distinct(sample),
              nHashes = n_distinct(Hash),
              nReads = sum(nReads), 
              Stage = paste0("Step_", round)) %>% 
    gather(starts_with("n"), value = "number", key = "Stat")
}

ASV.nested %>% 
  transmute(Miseq_run,Summary = map(Samples, ~ how.many(ASVtable = .,round = 0)))  -> ASV.summary

```

### Step 2: Model the composition of the positive controls of each run 


We create a vector of the composition of each positive control and substract it from the environmental samples from their runs



```{r jumping vector}


ASV.nested %>% 
  mutate (contam.tibble = map(Positives, 
                              function(.x){
                                .x %>%                    # Take the tibble from each run
                                  group_by(sample) %>%    
                                  mutate (TotalReadsperSample = sum(nReads)) %>%          # How many reads per sample 
                                  mutate (proportion = nReads/TotalReadsperSample) %>%    # What proportion does each Hash represent
                                  group_by (Hash) %>%
                                  summarise (vector_contamination = max (proportion))     # What is the maximum proportion that Hash ever gets
                                }) ) -> ASV.nested

ASV.nested %>% 
  unnest(contam.tibble) %>% 
  group_by(Miseq_run) %>% 
  summarise(nhash = n_distinct(Hash))# Check how it looks like



```


### Step 3: Substract the composition of the positive controls from the environment samples

The idea behind this procedure is that we know, for each run, how many reads from each Hash appeared in teh positive controls. These come from 2 processes: sequences we know should appear in the positive controls, and sequences that have *jumped* from the environment to the positive controls. With this procedure, we substract from every environmental sample the proportion of reads that jumped from elsewhere.

```{r cleaning step 1}
ASV.nested %>% 
  mutate(cleaned.tibble = map2(Samples, contam.tibble, function(.x,.y){  # We need two columns: the sample data and the contamination tibble
    .x %>%      # Get the samples tibble
      
      group_by (sample) %>%
      
      mutate (TotalReadsperSample = sum (nReads)) %>% # Calculate the number of reads per sample
      
      left_join(.y, by = "Hash")  %>%                  # Add, for each Hash shared with the positives, the value of the contamination vector
      # 
       mutate (Updated_nReads = case_when (!is.na(vector_contamination)    ~ (as.numeric(nReads) - (ceiling(vector_contamination*TotalReadsperSample))),
                                          
                                          TRUE                         ~ as.numeric(nReads)))  %>% # If the hash is present in the positives, substract the value of the vector times the total sample depth, otherwise, leave it as it is
      filter (Updated_nReads > 0) %>%       # Kepp only values > 0
      ungroup() %>%
      dplyr::select (sample, Hash, nReads = Updated_nReads)  # Leave no trace
      
    
  })) -> ASV.nested

ASV.nested %>% 
  unnest(cleaned.tibble) #Check how they look


```
Add this step to the summary table we were creating

```{r summary.file.2}
ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = map(cleaned.tibble, ~ how.many(ASVtable = .,round = "1.Jump"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 

# Check how does it look

ASV.summary %>% 
  unnest(Summary)

```

## Cleaning Process 2: **Discarding PCR replicates with low number of reads**

We will fit the number of reads assigned to each sample to a normal distribution and discard those samples with a probability of 95% of not fitting in that distribution. The output would be a dataset with less samples and potentially less number of unique Hashes.

```{r fitting nReads per sample}

ASV.nested %>% 
  unnest(cleaned.tibble) %>% 
  group_by(sample) %>%
  summarise(tot = sum(nReads)) -> all.reps

# Visualize

all.reps %>%  
  pull(tot) -> reads.per.sample

names(reads.per.sample) <- all.reps %>% pull(sample)  

normparams.reads <- MASS::fitdistr(reads.per.sample, "normal")$estimate   # Fit the number of reads per sample to a normal distribution



all.reps %>%  
  mutate(prob = pnorm(tot, normparams.reads[1], normparams.reads[2])) -> all.reps  # Probability of each depth to come from teh same distribution



outliers <- 
  all.reps %>% 
  filter(prob < 0.05 & tot < normparams.reads[1])    # Which sample are below 0.1

outliers %>% 
  arrange(tot) # Some have as few as 4k
  
outliers %>% 
  arrange(desc(tot))  # Max number of reads of a sample removed is 20k


ASV.nested %>% 
  mutate(Step.1.low.reads = map (cleaned.tibble, ~ filter(.,!sample %in% outliers$sample) %>% ungroup)) -> ASV.nested # Remove outliers

ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = map(Step.1.low.reads, ~ how.many(ASVtable = .,round = "2.Low.nReads"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 

```




## Cleaning Process 3: **Full clearance from Positive control influence**

Removing the Hashes that belong to the positive controls. First, for each Hash that appeared in the positive controls, determine whether a sequence is a true positive or a true environment. For each Hash, we will calculate, maximum, mean and total number of reads in both positive and samples, and then we will use the following decission tree:

  * If all three statistics are higher in one of the groups, we will label it either of Environmental or Positive control influence.
  
  * If there are conflicting results, we will use the Hashes. to see if they belong to either the maximum abundance of a Hash is in a positive, then it is a positive, otherwise is a real sequence from the environment.


Now, for each Hash in each set of positives controls, calculate the proportion of reads that were missasigned - they appeared somewhere they were not expected.
We will divide that process in two: first . A second step would be to create a column named proportion switched, which states the proportion of reads from one Hash that jumped from the environment to a positive control or viceversa. The idea is that any presence below a threshold can be arguably belong to tag jumping.

```{r real or positive}


ASV.table %>%                                         # Taking the whole dataset
  filter (Hash %in% list.of.hashes.in.positive) %>%   # keep only those hashes that have shown at least once in a positive control
  group_by(sample) %>% 
  mutate(tot.reads = sum(nReads)) %>% 
  group_by(Hash,sample) %>% 
  mutate(prop = nReads/tot.reads) %>% 
  group_by(Hash, source) %>% 
  summarise (max.  = max(prop),
             mean. = mean(prop),
             tot.  = sum(nReads)) %>% 
  gather(contains("."), value = "number", key = "Stat") %>%
  spread(key = "source", value = "number", fill = 0) %>% 
  group_by(Hash, Stat) %>%
  mutate(origin = case_when(Positives > Samples ~ "Positive.control",
                            TRUE                ~ "Environment")) %>% 
  group_by (Hash) %>%
  mutate(tot = n_distinct(origin)) -> Hash.fate.step2

# Do all three measurements agree

 Hash.fate.step2 %>% 
   filter(tot != 1 )

# No - remove only those for which the three measurements agree
 
 
Hash.fate.step2 %>% 
  filter(tot == 1) %>% 
  group_by(Hash) %>% 
  summarise(origin = unique(origin)) %>% 
  filter(origin == "Positive.control") -> Hashes.to.remove.step2

ASV.table %>% 
  group_by(source, Hash) %>% 
  summarise(ocurrences =n()) %>% 
  spread(key = source, value = ocurrences, fill = 0) %>% 
  #left_join(Hashes.to.remove.step2) %>% 
  #mutate(origin = case_when(is.na(origin) ~ "Kept",
   #                         TRUE          ~ "Discarded")) %>% 
  mutate(second.origin = case_when(Positives >= Samples ~ "Discarded",
                                   TRUE                 ~ "Kept")) %>% 
  filter(second.origin == "Discarded") %>% 
  full_join(Hashes.to.remove.step2) -> Hashes.to.remove.step2

```
IN order to train DADA2 to better distinguish when positive control sequences have arrived in the environment, we will keep the sequences in a csv file


```{r ASVs from positives}

Hashes.to.remove.step2 %>% 
  left_join(Hash.key) %>% 
  select(Hash, Sequence) %>% 
  write_csv("Hashes.to.remove.csv")

```

### Remove the positive control hashes from the composition of the ASVs

```{r cleaning.Step2}

ASV.nested %>% 
  mutate(Step2.tibble = map (Step.1.low.reads, ~ filter(.,!Hash %in% Hashes.to.remove.step2$Hash) %>% ungroup)) -> ASV.nested

saveRDS(ASV.nested, file = "Cleaning.before.Occ.model")

ASV.nested <- readRDS(file ="Cleaning.before.Occ.model")

ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = map(Step2.tibble, ~ how.many(ASVtable = .,round = "3.Positives"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 

ASV.summary %>% 
  unnest()
```

## Cleaning Process 4: **Occupancy modelling**

What is the probabilty of a true positive presence of a Hash in a Miseq Run. We will use eDNA occupancy modeling to asses whether a hash is a rare variant that spilled out or a true presence.

The process requires to load extra packages, create some model file, and group the hashes by Run, and biological replicate, summarising the data in a presence absence format.

The occupancy model itself was performed in the Rmarkdown file `Rjags.tunning.Rmd`, so here we will upload the csv file that contains all probability of occurences of all hashes per site. Each Hash-Site combination produces a matrix of presence abascences that feeds the model - for some cases it is a 30x3 matrix, for others it is a 39x3. We summarised the number of occurences in each case and run models for each unique case (to save computing time). Each unique model was run 10 times to filter out cases in which the model converge into a local maxima.

So we will import the object `Occ.fate.csv` and reduce the dataset to those Hashes with an occ > 0.8

```{r importing Occ results}

occ.results <- read_csv("Occ.fate.csv")

occ.results %>% 
  ggplot(aes(x = model)) +
  geom_histogram()

occ.results %>% 
  left_join(ASV.nested %>% 
              unnest(Step2.tibble) %>% 
              group_by(Hash) %>% 
              summarise (tot = sum(nReads))) %>% 
  ggplot(aes(x = cut_interval(model, n = 20))) +
  geom_col(aes(y = tot), fill = "red") 

```

So we will throw away most of the Hashes, but will keep most of the reads - we are getting into something here

```{r actual filtering}
 occ.results %>% 
  filter(model > 0.8) %>% 
  pull (Hash) -> to.keep

ASV.nested %>% 
  mutate(Step3.tibble = map (Step2.tibble, ~ filter(.,Hash %in% to.keep))) -> ASV.nested

ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = map(Step3.tibble, ~ how.many(ASVtable = .,round = "4.Occupancy"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 


```


## Cleaning Process 5: **Dissimilarity between PCR replicates**

So, a second way of cleaning the dataset is to remove samples for which the dissimilarity between PCR replicates exceeds the normal distribution of dissimilarities.
Sometimes the preparation of a PCR replicate goes wrong for a number of reasons - that leads to a particular PCR replicate to be substantially different to the other 2. In that case, we will remove the PCR replicate that has higher dissimilarity with the other two.

The process starts by adding the biological information to the ASV table, then diving the dataset by their biological replicate. This will also remove any sample that is not included in the metadata, eg coming from a different project.

```{r dissimilarity between PCR replicates}

ASV.nested %>% 
  unnest(Step3.tibble) %>%
  separate(sample, into = c(1:4,"rep", "run"), sep = "_", remove = F) %>%
  unite(`1`,`2`,`3`,`4`, run, col = "original_sample") -> cleaned.tibble
```


```{r quick check}
# do all samples have a name
cleaned.tibble %>% 
  filter (sample == "") # YES

# do all of them have an original sample
cleaned.tibble %>% 
  filter(original_sample == "") # YES

# do all of them have a Hash
cleaned.tibble %>% 
  filter(is.na(Hash)) # YES

# How many samples, how many Hashes
cleaned.tibble %>% 
  summarise(n_distinct(sample), # 325
            n_distinct(Hash))   # 3166

# Let's check the levels of replication

cleaned.tibble %>% 
  group_by(original_sample) %>% 
  summarise(nrep = n_distinct(sample)) %>% 
  #filter (nrep == 2) # 13
  filter (nrep == 1) # 4 
```
Ok, so there are 4 for which we only have 1 PCR replicate and 13 samples for which we only have 2 PCR replicates1.   We will get rid of those with only 1, as we can't estimate the PCR bias there.

```{r remove single replicates}
discard.1 <- cleaned.tibble %>% 
  group_by(original_sample) %>% 
  mutate(nrep = n_distinct(sample)) %>% 
  #filter (nrep == 2) # 25
  filter (nrep == 1) %>% 
  distinct(sample) %>% pull(sample)

cleaned.tibble %>% 
  filter(!sample %in% discard.1) -> cleaned.tibble
```

Anyway, let's have a visual representation of the dissimilarities between PCR replicates, biological replicates and everything else.

```{r lets do the PCR replication}
cleaned.tibble %>%
  group_by (sample) %>%
  mutate (Tot = sum(nReads),
          Row.sums = nReads / Tot) %>% 
  group_by (Hash) %>%
  mutate (Colmax = max (Row.sums),
          Normalized.reads = Row.sums / Colmax) -> cleaned.tibble
tibble_to_matrix <- function (tb) {
  
  tb %>% 
    group_by(sample, Hash) %>% 
    summarise(nReads = sum(Normalized.reads)) %>% 
    spread ( key = "Hash", value = "nReads", fill = 0) -> matrix_1
    samples <- pull (matrix_1, sample)
    matrix_1 %>% 
      ungroup() %>% 
    dplyr::select ( - sample) -> matrix_1
    data.matrix(matrix_1) -> matrix_1
    dimnames(matrix_1)[[1]] <- samples
    vegdist(matrix_1) -> matrix_1
}

tibble_to_matrix (cleaned.tibble) -> all.distances.full

#names(all.distances.full)


summary(is.na(names(all.distances.full)))
```

Let's make the pairwaise distances a long table
```{r}
as_tibble(as.matrix(all.distances.full) , rownames = "Sample1") %>% 
  gather(-Sample1, key = "Sample2", value = "value") -> all.distances.melted

summary(is.na(all.distances.melted$value))

# Now, create a three variables for all distances, they could be PCR replicates, BIOL replicates, or from the same site

all.distances.melted %>%
  separate (Sample1, into = c("Site1", "A", "B", "Month1", "rep", "round"), sep = "_", remove = FALSE) %>%
  unite (Site1,Month1, col = "Day.site1", remove = FALSE) %>%
  unite (Site1, Month1, A, B, round, col = "Bottle1", remove = F) %>% 
  separate (Sample2, into = c("Site2", "A2", "B2", "Month2", "rep2", "round2"), sep = "_", remove = FALSE) %>%
  unite (Site2, Month2, col = "Day.site2", remove = FALSE) %>%
  unite (Site2, Month2, A2, B2, round2, col = "Bottle2", remove = F) %>% 
  mutate ( Distance.type = case_when( Bottle1 == Bottle2 ~ "PCR.replicates",
                                      Day.site1 == Day.site2 ~ "Biol.replicates",
                                      Site1 == Site2 ~ "Same Site",
                                      TRUE ~ "Different Site"
                                     )) %>%
  dplyr::select(Sample1 ,Sample2  , value , Distance.type) %>%
  filter (Sample1 != Sample2) -> all.distances.to.plot

# Checking all went well

sapply(all.distances.to.plot, function(x) summary(is.na(x)))

all.distances.to.plot$Distance.type <- all.distances.to.plot$Distance.type  %>% fct_relevel( "PCR.replicates", "Biol.replicates", "Same Site")

  ggplot (all.distances.to.plot , aes (fill = Distance.type, x = value)) +
  geom_histogram (position = "dodge", stat = 'density', alpha = 0.9) +
 # facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance")
  
```



```{r}
# Instead of chosing based on the pw distances, we can do a similar thing using the distance to centroid

# Find out which samples have only two pcr replicates
cleaned.tibble %>% dplyr::select(-Miseq_run) %>% group_by(original_sample) %>% nest() -> nested.cleaning

nested.cleaning %>% 
  mutate(matrix = map(data, tibble_to_matrix)) -> nested.cleaning
nested.cleaning %>% mutate(ncomparisons = map(matrix, length)) -> nested.cleaning
 
  
dist_to_centroid <- function (x,y) {
  biol <- rep(y, length(x))
  
  if (length(biol) == 1) {
    output = rep(x[1]/2,2)
    names(output) <- attr(x, "Labels")
  }else{ 
    
  dispersion <- betadisper(x, group = biol)
  output = dispersion$distances
  }
  output
    }


nested.cleaning <- nested.cleaning %>% mutate (distances = map2(matrix, original_sample, dist_to_centroid))

unlist (nested.cleaning$distances) -> all_distances


```

```{r}
#normparams <- fitdistr(all_pairwise_distances, "normal")$estimate
normparams <- MASS::fitdistr(all_distances, "normal")$estimate
#  probs <- pnorm(all_pairwise_distances, normparams[1], normparams[2])
probs <- pnorm(all_distances, normparams[1], normparams[2])
outliers <- which(probs>0.95)

discard <-names (all_distances[outliers])

all_distances
to_write_discarded <- tibble(sample = discard, value = all_distances[outliers])

to_write_discarded <- to_write_discarded %>% bind_rows(tibble(sample = discard.1,
                                                              distance_to_centroid = NA))
write_csv(to_write_discarded ,"discared_samples.csv")


  
  
```

Finally, let's remove these samples from the dataset

```{r actual cleaning}

ASV.nested %>% 
  mutate(Step4.tibble = map (Step3.tibble,  ~ filter(.,! sample %in% to_write_discarded$sample))) -> ASV.nested

ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = map(Step4.tibble, ~ how.many(ASVtable = .,round = "5.PCR.dissimilarity"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 
```


## Exporting the output

We will export the final cleaned table with four columns (Miseq_run, sample, Hash, nReads)

```{r}

ASV.nested %>% 
  unnest(Step4.tibble) %>% 
  mutate(nReads = as.integer(nReads)) %>% 
  write_csv("Clean_data/ASV_table_all_together.csv")

ASV.nested %>% 
  unnest(Step4.tibble) %>% 
  distinct(Hash) %>% 
  left_join(Hash.key) %>% 
  write_csv("Clean_data/Hash_Key_all_together.csv")

library(tidyverse)
library(seqinr)

input <- read_csv("Clean_data/Hash_Key_all_together.csv")
output <- "Clean_data/Hash_Key_all_together.fasta"

write.fasta (sequences = as.list(input$Sequence),
             names = as.list(input$Hash),
             file.out = output)


saveRDS(ASV.nested, file = "ALL.ASVs.nested")


  

```


## Checking the output


Let's check out the success of our approach - use the taxonomy annotation to look for vertebrate sequences

```{r getting the taxonomy}

Taxonomy <- read_csv("../4.Taxonomic_annotation/Final.Classification.Hashes2018-11-06.csv")
Lineage <- read_csv("../../Analysis/Analysis.for.WSC/LineageLookupTable.families.csv")

ASV.nested %>% 
  unnest(Positives) %>% #          %>% summarise(sum(nReads))  50M reads passed cleaning filters
  left_join(Taxonomy) %>% 
  group_by(Family,Miseq_run) %>%
  summarise(tot = sum(nReads)) %>% 
  filter (str_detect(Family, "idae")) %>% 
  left_join(Lineage) %>% 
  arrange(desc(Phylum)) %>% 
  filter (Phylum == "Chordata") %>%  dplyr::select (Family,Miseq_run,tot) -> list.of.families.in.positives
  
ASV.nested %>% 
  unnest(Samples) %>% #filter (Miseq_run == 1) %>% group_by(sample) %>% summarise(tot = sum(nReads)) %>% filter (tot < 80000)#ggplot (aes(x = tot)) +geom_histogram(bins = 50)#          %>% summarise(sum(nReads))  50M reads passed cleaning filters
  left_join(Taxonomy) %>% 
  group_by(Family,Miseq_run) %>%
  summarise(tot = sum(nReads)) %>% 
  filter (str_detect(Family, "idae")) %>% 
  left_join(Lineage) %>% 
  arrange(desc(Phylum)) %>% 
  filter (Phylum == "Chordata") %>%  dplyr::select(Family,Miseq_run, tot) -> list.of.families.in.samples

ASV.nested %>% 
  unnest(Step3.tibble) %>% #          %>% summarise(sum(nReads))  50M reads passed cleaning filters
  left_join(Taxonomy) %>% 
  group_by(Family,Miseq_run) %>%
  summarise(tot = sum(nReads)) %>% 
  filter (str_detect(Family, "idae")) %>% 
  left_join(Lineage) %>% 
  arrange(desc(Phylum)) %>% 
  filter (Phylum == "Chordata") %>%  dplyr::select(Family,Miseq_run, tot) -> list.of.families.in.clean.samples

list.of.families.in.clean.samples %>% 
  group_by(Family) %>% 
  summarise(tot = sum (tot))

ASV.summary %>% 
  unnest() %>% #spread(Stat,  number)
  ggplot(aes(x=Stage, y=number, fill = Stat))+
    geom_line(aes(group = Stat, color = Stat))+
  #geom_boxplot(position = "dodge")+
  #facet_wrap(~Stat, scales = "free")+
  facet_grid(Stat~Miseq_run, scales = "free")+
  theme(axis.text.x = element_text(angle = 90, hjust =1))#,
                                 
# Sum stats for the paper - I need to do it for all runs at once, without nesting
ASV.nested %>% 
  filter (Miseq_run != 4) -> ASV.nested.to.paper

ASV.nested.to.paper %>% 
  unnest(Samples) %>% 
  ungroup() %>% 
  nest(.key = Samples) %>% 
  transmute(Summary = map(Samples, ~ how.many(ASVtable = .,round = 0)))  -> ASV.summary.all.together

ASV.nested.to.paper %>% 
  unnest(cleaned.tibble) %>% 
  ungroup() %>% 
  nest(.key = cleaned.tibble) %>% 
  transmute(Summary.1 = map(cleaned.tibble, ~ how.many(ASVtable = .,round = "1.Jump"))) %>% 
  bind_cols(ASV.summary.all.together) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary.all.together 

ASV.nested.to.paper %>% 
  unnest(Step.1.low.reads) %>% 
  ungroup() %>% 
  nest(.key = Step.1.low.reads) %>% 
  transmute(Summary.1 = map(Step.1.low.reads, ~ how.many(ASVtable = .,round = "2.Low.nReads"))) %>% 
  bind_cols(ASV.summary.all.together) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary.all.together 

ASV.nested.to.paper %>% 
  unnest(Step2.tibble) %>% 
  ungroup() %>% 
  nest(.key = Step2.tibble) %>% 
  transmute(Summary.1 = map(Step2.tibble, ~ how.many(ASVtable = .,round = "3.Positives"))) %>% 
  bind_cols(ASV.summary.all.together) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary.all.together 

ASV.nested.to.paper %>% 
  unnest(Step3.tibble) %>% 
  ungroup() %>% 
  nest(.key = Step3.tibble) %>% 
  transmute(Summary.1 = map(Step3.tibble, ~ how.many(ASVtable = .,round = "4.Occupancy"))) %>% 
  bind_cols(ASV.summary.all.together) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary.all.together 


ASV.nested.to.paper %>% 
  unnest(Step4.tibble) %>% 
  ungroup() %>% 
  nest(.key = Step4.tibble) %>% 
  transmute(Summary.1 = map(Step4.tibble, ~ how.many(ASVtable = .,round = "5.PCR.dissimilarity"))) %>% 
  bind_cols(ASV.summary.all.together) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary.all.together 

ASV.summary.all.together %>% 
  unnest() %>% 
  ggplot(aes(x=Stage, y=number, fill = Stat))+
    geom_line(aes(group = Stat, color = Stat)) +
  facet_wrap(.~Stat, scales = "free")+
  theme(axis.text.x = element_text(angle = 45, hjust =1),
        axis.title.y = element_blank())
  

ASV.summary.all.together %>% 
  unnest() %>% 
  spread(key = Stat, value = number)
23.558632/28.289321

```

```{r}

ASV.summary %>% 
  unnest() %>% 
  filter (Miseq_run!= "4") %>% 
  group_by(Stage, Stat) %>%
  summarise(number = sum(number)) %>% 
  spread(key = Stat , value = number)
  nest ()

#where are the salmons

ASV.nested %>% 
  unnest(Step4.tibble) %>% #        
  left_join(Taxonomy) %>% 
  separate(sample, into = "biol", sep = "\\.", remove = F) %>% 
  group_by(Family, biol) %>% 
  summarise (tot = sum(nReads)) %>% 
  filter (Family == "Salmonidae") %>% 
  arrange(desc(tot))

# Prevalence plot as in decontam package

ASV.table %>% 
  group_by(source, Hash) %>% 
  summarise(ocurrences =n()) %>% 
  spread(key = source, value = ocurrences, fill = 0) %>% 
  left_join(Hashes.to.remove.step2) %>% 
  mutate(origin = case_when(is.na(origin) ~ "Kept",
                            TRUE          ~ "Discarded")) %>% 
  ggplot(aes(x = Positives, y = Samples, color = origin))+
  geom_point()

ASV.table %>% 
  group_by(source, Hash) %>% 
  summarise(ocurrences =n()) %>% 
  spread(key = source, value = ocurrences, fill = 0) %>% 
  #left_join(Hashes.to.remove.step2) %>% 
  #mutate(origin = case_when(is.na(origin) ~ "Kept",
   #                         TRUE          ~ "Discarded")) %>% 
  mutate(second.origin = case_when(Positives >= Samples ~ "Discarded",
                                   TRUE                 ~ "Kept")) %>% 
  filter(second.origin == "Discarded") %>% 
  full_join(Hashes.to.remove.step2) -> Hashes.to.remove.step2
Hashes.to.remove.step2 %>% 
filter (Hash == "acebcd5c491bb273f3e4d615cafad649")
```


